# 2.7 结构化的回归模型

## 1 问题的困难度

* 对任意函数 $f$,考虑 $RSS$ 准则
  $$
  RSS(f) = \sum_{i=1}^N (y_i-f(x_i))^2
  $$
  如果在每个 $x_i$ 值处有多个观测对$(x_i,y_{il}),l=1,…,N_i$，风险会被限制．在这种情形下，解会过每个 $xi​$ 对应的所有 $y_{il} 的平均值。2.4 统计判别理论中已经讨论到了，同时可以看Ex 2.6。

* 一般地，大多数学习方法施加的约束条件都可以描述为这样或那样对复杂度的限制．这通常也意味着在输入空间的小邻域的一些规则的行为．这就是，对于在某种度量下充分互相接近的所有输入点 $x$，$f$ 展现了一些特殊的结构比如说接近常值，线性或者低次的多项式．然后通过在邻域中平均或者进行多项式拟合得到估计量。

* 任何在 **各向同性的 (isotropic)** 邻域中试图产生局部变化的函数会在高维中遇到问题——还是维数灾难．相反地，所有克服维数问题的方法在衡量邻域时有一个对应的度量（经常是隐式的或者自适应的），该度量的基本要求是不允许邻域在各个方向都同时小．

## 2 练习

### Ex 2.6

* 题目：![selection_080](https://user-images.githubusercontent.com/13688320/53959044-8d2e7580-411d-11e9-8ddf-793cf115b914.png)

* 题目大意：最小二乘法中若存在重复数据，则普通最小二乘可以看成是 **（样本量）减少的加权最小二乘 (reduced weighted least squares)**．

* 知识补充：

  > * 普通最小二乘法
  >
  >   * 普通最小二乘法是应用最广泛的一种最小二乘法，它的目标是求得一个使得全局残差平方和最小的参数。假设有$N$个样本$x=(x_1,x_2,...,x_N)$，普通最小二乘法要求残差$\epsilon=(\epsilon_1, \epsilon_2, \dots,\epsilon_N)$满足$Gauss-Markov$假设：
  >    $$
  >     E(\epsilon) = \mathbf{0}\\
  >     Cov(\epsilon) = \sigma^2\mathbf{I}\\
  >     Cov(\epsilon, x) =\mathbf{0}
  >    $$
  >    
  >   * 优化目标
  >    $$
  >     RSS(\beta) = (y-X^T\beta)^T(y-X^T\beta)
  >    $$
  >    
  >
  > * 加权最小二乘
  >
  >   * 加权最小二乘法和普通最小二乘法形式类似，只是残差不满足$Gauss-Markov$假设，其残差的协方差不要求是单位矩阵，而是对角阵，且对角线的值不一定相等。
  >    $$
  >     E(\epsilon) = \mathbf{0}\\
  >     Cov(\epsilon) = \sigma^2\mathbf{D}\\
  >     Cov(\epsilon, x) =\mathbf{0}
  >    $$
  >     其中$\mathbf{D}$是对角阵而不是单位阵$\mathbf{I}​$
  >
  >     * 优化目标
  >    $$
  >       RSS(\beta) = (y-X^T\beta)^T\mathbf{W}(y-X^T\beta)
  >    $$
  >       其中$W$是对角阵
  >
  >   * 求解
  >    $$
  >     dRss(\beta)
  >     \\ = d(y-X^T\beta)^TW(y-X^T\beta)+(y-X^T\beta)^TWd(y-X^T\beta)
  >     \\ = tr(d\beta^TXW(y-X^T\beta)) + tr((y-X^T\beta)^TWX^Td\beta)
  >     \\ = tr(2(XW(y-X^T\beta))^Td\beta)
  >     \\ \frac{\part Rss}{\part \beta} =2(XW(y-X^T\beta))=0
  >     \\ XWy=XWX^T\beta
  >     \\ \beta = (XWX^T)^{-1}XWy
  >    $$

* 解：设$X_u=(X_{u1}, X_{u2},\dots,X_{uN})​$是$X=(X_{1}, X_{2},\dots,X_{M})​$去重之后的矩阵

  * 假设
    $$
    X^T=A_{MN}X_u^T\\
    y_u=W^{-1}A^Ty
    $$
    假设的含义为：其中$A$是每行只要1一个为1，其他全为0的矩阵；$W$是对角阵，且$W_{i,i}$是$X_{ui}$的重复数，$W=A^TA$；$y_u$取重复$y$的均值

  * $A$一定是列满秩的，则存在右逆（广义逆），设A的右侧逆为$A_R^{-1}$

  * $W$是对角阵，则一定存在唯一逆矩阵，且容易得到
    $$
    W^{-1}=A_R^{-1}{A_R^{-1}}^T\\
    \label{eq1}
    $$
    

  * W^{-1}={A_R^{-1}}^T $

  * 对于普通的最小二乘法，优化目标为
    $$
    (y-X^T\beta)^T(y-X^T\beta)
    \\ = (A^Ty-A^TX^T\beta)^TA_R^{-1}{A_R^{-1}}^T(A^Ty-A^TX^T\beta)
    \\ = (A^Ty-A^TX^T\beta)^TW^{-1}(A^Ty-A^TX^T\beta) \quad \because equation \ \ref{eq1}
    \\ = (Wy_u-WX_u^T\beta)^TW^{-1}(Wy_u-WX_u^T\beta)
    \\ = (y_u-X_u^T\beta)^TW(y_u-X_u^T\beta)
    $$

  * 可见与加权优化目标一致，证毕

  

  