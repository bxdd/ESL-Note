# 2.8 限制估计的种类

## 1 粗糙度惩罚和贝叶斯方法

* 由显式的惩罚 $RSS(f)$ 以及粗糙度惩罚控制的函数类别
  $$
  PRSS(f;\lambda)=RSS(f)+\lambda J(f)
  $$

* 举例：三次光滑样条
  $$
  PRSS(f;\lambda)=\sum_{i=1}^N (y_i-f(x_i))^2
  $$
  这里的粗糙惩罚控制了$f$ 的二阶微分较大的值，而且惩罚的程度由 $\lambda \ge 0$ 来决定。$\lambda=0$ 表示没有惩罚，则可以使用任意插值函数，$\lambda=\infin$仅仅允许关于 $x$ 的线性函数。

* 举例：可加性函数
  $$
  f(X)=\sum_{j=1}^pf_j(X_j), J(f)=\sum_{j=1}^p J(f_j)
  $$
  可加性惩罚与可加性函数 联合使用去构造可加的光滑坐标函数的模型

* 举例：投影映射回归 (Projection Pursuit Regression)，见[投影映射回归](..\A 基础知识\A.2 数值方法\3 投影映射回归)
  $$
  f(x)=\sum_{i=1}^mS_{\alpha_i}(\alpha_i^Tx)
  $$
  其中 $\alpha_i$为自适应选择的方向，每个函数 $S_{\alpha_i}$ 有对应的粗糙惩罚 (?)。

* 惩罚函数，或者说 **正则 (regularization)** 方法，表达了我们的 **先验信仰 (prior belief)**。寻找具有一个特定类型的光滑行为函数类型，而且确实可以套进贝叶斯的模型中．惩罚 $J$ 对应先验概率，$PRSS(f;\lambda)$ 为后验分布，最小化 $PRSS(f;\lambda)$意味着寻找后验模式



## 2 核方法和局部回归

