# 2.8 限制估计的种类

## 1 粗糙度惩罚和贝叶斯方法

* 由显式的惩罚 $RSS(f)$ 以及粗糙度惩罚控制的函数类别
  $$
  PRSS(f;\lambda)=RSS(f)+\lambda J(f)
  $$

* 举例：三次光滑样条
  $$
  PRSS(f;\lambda)=\sum_{i=1}^N (y_i-f(x_i))^2
  $$
  这里的粗糙惩罚控制了$f$ 的二阶微分较大的值，而且惩罚的程度由 $\lambda \ge 0$ 来决定。$\lambda=0$ 表示没有惩罚，则可以使用任意插值函数，$\lambda=\infin$仅仅允许关于 $x$ 的线性函数。

* 举例：可加性函数
  $$
  f(X)=\sum_{j=1}^pf_j(X_j), J(f)=\sum_{j=1}^p J(f_j)
  $$
  可加性惩罚与可加性函数 联合使用去构造可加的光滑坐标函数的模型

* 举例：投影映射回归 (Projection Pursuit Regression)，见[投影映射回归](..\A 基础知识\A.2 数值方法\3 投影映射回归)
  $$
  f(x)=\sum_{i=1}^mS_{\alpha_i}(\alpha_i^Tx)
  $$
  其中 $\alpha_i$为自适应选择的方向，每个函数 $S_{\alpha_i}$ 有对应的粗糙惩罚 (?)。

* 惩罚函数，或者说 **正则 (regularization)** 方法，表达了我们的 **先验信仰 (prior belief)**。寻找具有一个特定类型的光滑行为函数类型，而且确实可以套进贝叶斯的模型中．惩罚 $J$ 对应先验概率，$PRSS(f;\lambda)$ 为后验分布，最小化 $PRSS(f;\lambda)$意味着寻找后验模式



## 2 核方法和局部回归

* 概念：通过确定局部邻域的本质来显式给出回归函数的估计或条件期望，并且属于局部拟合得很好的规则函数类，可视为k近邻算法（K-Nearest Neighbour, KNN）的推广。

* 在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与线性回归类似，在这个子集上基于最小均方误差来进行普通的回归
  $$
  RSS(f_{\theta},x_0)=\sum_{i=1}^NK_{\lambda}(x_0,x_i)(y_i-f_{\theta}(x_i)^2)
  $$

* 类似于KNN每次预测都需要实现选出对应的数据子集，本算法使用使用“核”来对附近的点赋予更高的权重，其中$K_{\lambda}(x_0,x_i)​$就是**核函数(kernel function)**，类如常用的高斯核：
  $$
  K_{\lambda}(x_0,x_i)= \frac{1}{\lambda}\exp{(\frac{\|x_i-x_0\|^2}{2\lambda})}
  $$

* 例如：Nadaraya-Watson 回归，核估计的最简单形式，此时$f(x)$的形式是$f(x_0)=\theta$
  $$
  \hat f(x_0)=\frac{\sum_{i=1}^NK_{\lambda}(x_0,x_i)y_i}{\sum_{i=1}^NK_{\lambda}(x_0,x_i)}
  $$
  

* 例如：局部加权线性回归 (Locally Weighted Linear Regression，LWLR)，此时$f(x)$的形式为$f(x)=\theta_0+\theta_1^Tx$，利用[加权线性最小二乘法](..\A 基础知识\A.2 数值方法\1 加权最小二乘法)求解

* 例如：k-近邻方法，此时核函数可以看作 ($x_{(k)}$: 距离$x_0$第$k$近的点，$I(S)$: 集合$S$的指标函数)
  $$
  K_k(x,x_0)=I(\|x-x_0\|\le\|x_{(k)}-x_0\|)
  $$
  

## 3 基函数和字典方法

* 线性和多项式展开式或者多种多样的更灵活的模型。这些关于 $f​$ 的模型是基本函数的线性展开
  $$
  f_{\theta}(x)=\sum_{m=1}^M\theta_mh_m(x)
  $$
  其中每个 $h_m$ 是输入 $x$ 的函数，并且其中的线性项与参数 $\theta​$ 的行为有关

