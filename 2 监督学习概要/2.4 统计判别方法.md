# 2.4 统计判别理论

## 1 预测值为连续值

### 1.1 平方误差损失

* 公式
  $$
  L(Y,f(X))=(Y-f(X))^2
  $$

* 预测平方误差的期望
  * 公式
    $$
    EPE(f) = E(Y-F(X))^2
    \\=\int [y-f(x)]^2Pr(x,y)dxdy
    \\=E_XE_{Y|X}([Y-f(X)]^2)
    $$

  * 则只需要EPE逐点最小
    $$
    f(x) = \mathop{\arg\min}_{c}E([Y-c]^2|X=x) \\
    f(x) = E(Y|X=x)
    $$

* k-近邻

  * 联合概率分布 $Pr(X,Y)$ 温和正则条件下，可以证明当 $N,k\rightarrow \infin$使得$k/N\rightarrow 0$ 时，$\hat f (x) \rightarrow E(Y|X=x)$
  * 当维数 p 变大，k-最近邻的度量大小也随之变大．所以最近邻代替条件会让我们非常失望

* 线性模型

  * 设X，Y服从某个分布$Pr(X,Y) $, 推到得出$\beta​$ 如下

    * $E([Y-X^T\beta]^2)=E_XE_Y([Y-X^T\beta]^2|X=x)$

    * $$
      \hat\beta=\mathop{\arg\min}_{\beta}(E([Y-X^T\beta]^2))
      \\=\mathop{\arg\min}_{\beta}\int\int[Y-X^T\beta]^2Pr(X,Y)dXdY
      
      \\F = [Y-X^T\beta]^2
      \\dF = d[Y-X^T\beta]^2
      \\=tr(-X^Td\beta (Y-X^T\beta)-(Y-X^T\beta)X^Td\beta)
      \\=tr(-2(Y-X^T\beta)X^Td\beta)
      $$

      

    * $$
      \frac{\part F}{\part \beta} = -2X(Y-X^T\beta)=0\\
      \frac{\part E(F)}{\part \beta}= \int \int \frac{\part F}{\part \beta} Pr(X,Y)dXdY
      \\=\int \int -2X(Y-X^T\beta) Pr(X,Y) dXdY=0\\
      \beta = E(XX^T)^{-1}E(XY)
      $$

  * 最小二乘法相当于把期望换成了样本的均值，也就是假设了分布和样本相同

    * $$
      \beta = (XX^T)^{-1}X\mathbf{y} \\
      X = (\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_n})
      $$

    * $$
      (XX^T)^{-1}
      \\=\left[(\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_n})
      \left(\begin{matrix}
      \mathbf{x_1}^T\\
      \mathbf{x_2}^T\\
      \vdots\\
      \mathbf{x_n}^T
      \end{matrix}
      \right)
      \right]^{-1}
      \\=\left[n\times\frac{\sum_{i=1}^n\mathbf{x_i}\mathbf{x_i}^T}{n}\right]^{-1}
      \\ \approx \frac{1}{n}[E(\mathbf{x}\mathbf{x}^T)]^{-1}
      $$

    * $$
      X\mathbf{y} = (\mathbf{x_1},\mathbf{x_2},...,\mathbf{x_n})
      \left(\begin{matrix}
      y_1\\
      y_2\\
      \vdots\\
      y_n
      \end{matrix}
      \right)
      \\=n\sum_{i=1}^n\frac{\mathbf{x_i}y_i}{n}
      \\ \approx nE(\mathbf{x}y)
      $$

  

### 1.2 绝对值误差损失

* 公式
  $$
  EPE(f) = E|Y-F(X)|
  \\ =\int \int |Y-F(X)|Pr(X,Y) dYdX
  \\ f(x) = \mathop{\arg\min}_{c}E(|Y-c||X=x) =\int_{a}^{b}|Y-c|P(Y|X=x)dY\\
  $$

* 一定有
  $$
  c =Y_{1/2}\\
  \int_{-\infin}^{c}P(Y|X=x)dY=\frac{1}{2}
  $$

* TODO: 证明:假设下届为a，否则为无穷积分不好证明。
  $$
  \int_{a}^{c}|Y-c|P(Y|X=x)dY\\
  = \lim_{n\rightarrow \inf} \sum_{i=0}^{n+1}(y_{i}-y_{i-1})P(y_{i                                                          }|X=x)|y_i-c|
  $$

* 它的估计比条件均值更加鲁棒

## 2 贝叶斯分类

### 2.1 损失函数

* 公式
  $$
  EPE=E[L(G,\hat G(X))]
  \\ = E_X\sum_{k=1}^K L(G_k,\hat G(X))Pr(G_x|X)
  $$

* 求解

  * 只需要逐点最小

  $$
  \hat G(x) = \mathop{\arg\min}_{g\in G}\sum_{k=1}^KL(G_k,g)Pr(G_k|X=x)
  $$


  * 如果$L$为 0-1 损失函数，则为贝叶斯分类 (Bayes classifier)

    * **贝叶斯阶 (Bayes rate)**：贝叶斯分类的误差阶

    * 
        $$
        \hat G(x)=\mathop{\arg\min}_{g\in G} \sum_{k=1}^KL(G_k,g)Pr(G_k|X=x)
        \\ =\mathop{\arg\min}_{g\in G} \sum_{G_k\not= g}L(G_k,g)Pr(G_k|X=x)
        \\=\mathop{\arg\min}_{g\in G}(1- Pr(g|X))
        \\ = \mathop{\arg\max}_{g\in G} Pr(g|X)
        $$

  *  k-最近邻分类直接近似这个解决方法——在最近邻内占绝大多数恰好意味着这个

    * 某一点的条件概率松弛为该点的邻域内的条件概率
    * 概率是通过训练样本的比例来估计的．

### 2.2 贝叶斯分类求解

* 问题：我们从双变量高斯分布 $N((1,0)T,I)$ 中产生 10 个均值 $m_k$，标记此类为蓝色．类似地，从双变量高斯分布 $N((0,1)T,I)$ 中产生 10 个均值并标记此类为橘黄色．然后对每一类按照下面方式产生 100 个观测：对于每个观测，我们以 $1/10$ 的概率随机选择一个 $m_k$，然后产生 $N(m_k,I/5)$，因此对于每一类引出了一个高斯分布簇的混合．

* 求解：

  * 假设蓝色点分布随机变量为X, 黄色为Y

  * 根据条件概念
    $$
    P(X=x)=\sum_{i=1}^{10}P(X=x|choose=m_i)P(choose=m_i)
            \\=\sum_{i=1}^{10}\frac{P(X=x|choose=m_i)}{10}
            \\=\frac{1}{10}\sum_{i=1}^{10} \int P(X_i=x,M_i=m_i) dm_i{}\\
            \\ = \frac{1}{10}\sum_{i=1}^{10} \int P(X_i=x|M_i=m_i)P(M_i=m_i)dm_i\\
    $$

  * 其中
    $$
    X_i\sim N(m_i,I/5)
    $$
    

  * 则有
    $$
    P(X_i=x|M_i=m_i)=\frac{1}{2\pi}\exp(-\frac{1}{10}(x-m_i)^T(x-m_i))\\
    P(X_i=x)=\int_{D} \frac{1}{2\pi}\exp(-\frac{1}{10}(x-m_i)^T(x-m_i))P(M_i=m_i) dm_i
    $$

  * 其中
    $$
    M_i\sim N((1,0)^T,I)
    $$

  * 由此可以求得$P(X=x)$的数值解，最终可以比较$P(X=x)$和$P(Y=x)$ ,从而可以确定区域为

    ![1608571213678](assets/1608571213678.png)