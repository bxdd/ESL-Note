# 统计模型，监督学习和函数逼近

## 1 联合分布 $Pr(X,Y)$ 的统计模型

* 可加误差模型

  * 公式
    $$
    Y=f(X)+\epsilon
    $$

  * 其中随机误差 $\epsilon$ 满足 $E(\epsilon)=0$ 且与 $X$ 独立

  * 大多数系统输入输出对 $(X,Y)$没有一个确定的关系 $Y=f(X)$。一般地，存在不可测量的变量对 $Y$ 起作用，包括测量误差．可加误差模型假设我们可以通过误差 $\epsilon$ 从确定关系中捕捉所有的偏移量。

* 一般地，条件分布$Pr(Y∣X) $可以以某种复杂的方式依赖 $X​$，但是可加误差模型排除了这些情形。(TODO 不懂)

## 2 监督学习

* 学习算法能够根据原始输出和产生的输出之间的差异 $y_i - \hat f(x_i)$ 来修改输入和输出的关系 $\hat f$

## 3 函数逼近

* 线性基展开

* 非线性展开

* 对于线性模型我们得到该最小化问题的一个简单的 **闭型 (closed)** 解．如果基本函数本身没有任何隐藏的参数，这种方法也适用．否则这种解决方法不是需要迭代的方法就是需要数值优化．

  > closed-form 是指可以进行有限次赋值的表达式，其中可能包含常数、变量、常见的运算符（加减乘除）或函数（指数、对数、三角函数），但是通常不存在极限运算。依此理解，粗略地说，有显式解的应当为 closed form solution．

* 极大似然估计

  * 假设我们有一个指标为 $\theta$ 的密度为 $Pr_{\theta}(y)$ 的随机样本 $yi,i=1,…,N$。观测样本的概率对数值为$L(θ)=\sum_{i=1}^{N}\log Pr_{\theta}(y_i)$，极大似然的原则是假设最合理的 $\theta ​$ 值会使得观测样本的概率最大。

  * 可加误差模型:  $Y=f_{\theta}(X)+\epsilon,\epsilon \sim N(0,\sigma^2)​$ 的最小二乘，等价于使用下面条件概率的极大似然
    * 公式 
      $$
      Pr(Y|X,\theta)=N(f_{\theta}(X),\sigma^2)
      $$

    * 证明
      $$
      L(\theta)=\sum_{i=1}^N \log\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y_i-f_{\theta}(x_i))^2}{2\sigma^2}\right)
      \\ = \sum_{i=1}^N  -\frac{1}{2}\log{2\pi} - \log\sigma-\frac{1}{2\sigma^2}(y_i-f_{\theta}(x_i))^2
      \\ = -\frac{N}{2}-N\log\sigma -\frac{1}{2\sigma^2}\sum_{i=1}^N(y_i-f_{\theta}(x_i))
      $$
      

  * 分类模型：给定 $X, G=\{g_1,g_2,\dots g_K\}$知道每一类的条件概率为 $Pr(G=g_i∣X=x)​$。则极大似然等于使得交叉熵最大

    * 证明
      $$
      L(\theta)=\sum_{i=1}^N \log Pr(G=g_i|X=x_i)
      \\ = \sum_{i=1}^N \sum_{g=1}^KPr(g_i=g)\log Pr(G=g|X=x_i)
      $$
      
