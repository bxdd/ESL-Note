# 4 收缩的方法

## 1 概述

* **子集选择 (subset selection)** 可得到一个可解释的、预测误差可能比全模型低的模型。然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差。
* **收缩方法 (shrinkage methods)** 更加连续，因此不会受 **高易变性 (high variability)** 太大的影响．

## 2 岭回归

### 2.1 概念与公式

* 概念：**岭回归 (Ridge regression)** 根据回归系数的大小加上惩罚因子对它们进行收缩。岭回归的系数使得带惩罚的残差平方和最小

* 公式：
  $$
  P_{\lambda}:\hat\beta^{ridge}=\arg\min_{\beta}\left\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j+\lambda\sum_{j=1}^p \beta_j^2)\right\},\ \lambda \ge 0
  $$
  $\lambda$ 值越大，收缩的程度越大．每个系数都向零收缩。通过参数的平方和来惩罚的想法也用在了神经网络，也被称作**权重衰减 (weight decay)**

* 拉格朗日等价：

  岭回归问题可以等价地写成，而且$\lambda$和$t$有一一对应的关系

  * 公式
    $$
    Q_{t}: \overline \beta^{ridge}=\arg\min_{\beta} \sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p \beta_j x_{ij} )^2\\
    subject\ to \sum_{i=1}^p \beta_j^2\le t
    $$

  * 等价的定义

    * 正推：对$\forall \lambda\ge 0​$, 都$\exists t \ge 0​$, 使得$\hat\beta^{ridge}=\overline \beta^{ridge}​$  
    * 反推：对$\forall t \ge 0​$, 都$\exists\lambda \ge 0​$, 使得$\overline \beta^{ridge}=\hat\beta^{ridge}​$ 

  * 证明(为了简化，这里假设不存在$\beta_0$, 即无截距)：

    * 对于$P_{\lambda}​$，有
      $$
      \frac{\part (y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta}{\part \beta} =0
      \\ \leftrightarrow X^T(X\beta - y) + \lambda \beta= 0
      $$

    * 对于$Q_{t}​$

      * 首先可以化为矩阵形式
        $$
        Q_{t}: \overline \beta^{ridge}=\arg\min_{\beta} (y-X\beta)^T(y-X\beta)\\
        subject\ to\ \beta^T\beta-t\le 0
        $$

      * 由拉格朗日函数法
        $$
        L(\beta,\mu)=(y-X\beta)^T(y-X\beta)+\mu(\beta^T\beta-t)
        $$

      * 根据$KTT​$条件
        $$
        \left\{
        \begin{align}
        &\nabla f + \mu \nabla g = 0\\
        &\ g(x) \le 0\\
        &\mu >= 0\\
        &\mu g(x) = 0
        &\end{align}
        \right.
        $$
        有
        $$
        \left\{
        \begin{align}
        &X^T(X\beta - y) + \mu \beta=0\\
        &\ \beta^T\beta-t\le 0\\
        &\mu >= 0\\
        &\mu(\beta^T\beta-t) = 0
        &\end{align}
        \right.
        $$

    * 首先证明正推：

      * 对$\forall \lambda\ge 0$，可得$P_{\lambda}$ 的解为
        $$
        \hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty
        $$

      * 只需要使得$Q_{t}$中
        $$
        \mu=\lambda\\
        t=((X^TX+\lambda I)^{-1}X^Ty)^T(X^TX+\lambda I)^{-1}X^Ty
        $$

      * 即可使得两者解相同

    * 然后证明反推：

      * 对$\forall t \ge 0 ​$， 取$\mu​$ 是满足$KTT​$ 条件的一个值

        * 若$t\ge ((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty, 此时取​$$\mu =0 ​$即可满足$KTT​$条件，则也取$\lambda =0​$ 即可

        * 若$t< ((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty​$, 则必有
          $$
          \mu !=0\\
          t=\beta^T\beta
          $$
          取$\mu$取满足下式的一个值即可
          $$
          ((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty=t\\
          \label{eq2}
          $$

          * 因为

            $$
            (1)\lim_{\mu\rightarrow\infin}((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty
            \\=((\mu I)^{-1}X^Ty)^T(\mu I)^{-1}X^Ty=0<t\\
            (2)when_{\mu=0}((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty
            \\=((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty>t
            $$
          * 由于函数连续性，所以则必定$\exists \mu$, 满足式子 $\eqref{eq2}$

          此时也取$\lambda=\mu$即可

* 为什么这么做：当在线性回归模型中有许多相关变量，它们的系数可能很难确定且有高方差．某个变量的较大的正系数可以与相关性强的变量的差不多大的负系数相互抵消。通过对系数加入大小限制，问题可以减轻

* 矩阵形式

  * 输入标准化：

    * 由于对输入按比例进行缩放时，岭回归的解不相等，因此求解前我们需要对输入进行标准化。

    * 注意到惩罚项不包含截距 $\beta_0$，因为对截距的惩罚会使得过程依赖于 $y$ 的初始选择（每个 $y_i$ 加上常数 $c$ 不是简单地导致预测值会偏离同样的量 $c​$）

    * 可以证明，当对输入进行中心化后（即$\mathbf{x}_{norm}=\mathbf{x}-\bar{\mathbf{x}}​$)，$Q_t​$的解可以分为两部分

      * 使用$\bar y=\frac{\sum_{i=1}^N y_i}{N}$来估计$\beta_0$
      * 使用无截距的岭回归，利用中心化的$\mathbf{x}$估计剩余部分参数

      

    证明请看[习题Ex 3.5](A 习题)

  * 公式
    $$
    RSS(\lambda)=(y-X\beta)^T(y-X\beta)+\lambda\beta^T\beta
    $$
    因此，岭回归可以写作如上，其中$X$是$p$列，每一行都是经过中心化了的输入变量。而$y$也是传统岭回归的$y_{norm}=y-\beta_0=y-\bar y$中心化的值

    解：
    $$
    \frac{\part (y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta}{\part \beta} =0
    \\ \leftrightarrow X^T(X\beta - y) + \lambda \beta= 0
    \\ \leftrightarrow \hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty
    $$

  * 由岭回归矩阵形式的解$\hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty$可以看到，即使$X^TX$不是满秩的，$X^TX+\lambda I$也是满秩矩阵，这样会使得问题非奇异，而且这是第一次将岭回归引入统计学中的主要动力

    * 证明：
      $$
      \forall x >0, x^T(X^TX+\lambda I)x=(Xx)^TXx+\lambda x^Tx > 0
      $$
      因此$X^TX+\lambda I​$正定，正定矩阵必定满秩，证毕

### 2.2 岭回归与后验分布

* 当给定一个合适的先验分布，岭回归也可以从后验分布的均值或众数得到

* 假设

  * $y$的先验分布为 $N(\beta_0+X\beta,\sigma^2I)$
  * 参数 $\beta$ 的前验分布为 $N(0,\tau^2I)$
  * 参数$\beta_0$不包含任何前验信息，即前沿分布是均匀分布

* 则$\beta $ 后验分布密度函数的对数值就是$ridge$

* 证明

  

  * 由后验概率公式，可得
    $$
    P([\beta_0,\beta]|y) = \frac{P([\beta_0,\beta],y)}{P(y)} = \frac{P(y|[\beta_0,\beta])P([\beta_0,\beta])}{P(y)}  \\\sim P(y|[\beta_0,\beta])P(\beta)P(\beta_0) \sim P(y|[\beta_0,\beta])P(\beta) 
    \\=\frac{1}{\sqrt{(2\pi\sigma^2)^{N}}}\exp(-\frac{(y-\beta_0-X\beta)^T(y-\beta_0-X\beta)}{2\sigma^2})\times \frac{1}{\sqrt{(2\pi\tau)^{N}}}\exp(-\frac{\beta^T\beta}{2\tau^2})\\
    =C\exp(-\frac{1}{2\sigma^2}((y-\beta_0-X\beta)^T(y-\beta_0-X\beta)+\frac{\sigma^2}{\tau^2}\beta^T\beta))
    $$

  * 取负对数
    $$
    -\log P([\beta_0,\beta]|y)=-\log C+\frac{1}{2\sigma^2}((y-\beta_0-X\beta)^T(y-\beta_0-X\beta)+\frac{\sigma^2}{\tau^2}\beta^T\beta)
    $$

  * 因为后验分布仍然是高斯分布，因此是众数也是均值

### 2.3 岭回归与奇异值分解

* 中心化输入矩阵$X_{N\times p}​$进行**奇异值分解 (SVD)** 可以进一步了解了岭回归
  $$
  X_{N\times p}=U_{N\times p}D_{p\times p}V_{p\times p}^T
  $$

  * 这是对标准奇异值分解进行矩阵分块得到的，称为Thin SVD（请参考[奇异值分解](..\A 基础知识\A.1 代数理论\2 协方差矩阵与矩阵分解)), $U$张成$X$的列空间，$V$张成$X$的行空间

  * $D_{p\times p}​$是对角阵，其对角元$d_1\ge d_2\ge \dots \ge d_p \ge 0​$, 若有$d_j=0​$，则称$X​$为奇异的。其实若$X​$列满秩，则$D​$也是非奇异对角阵

    

* 利用奇异值分解，通过简化我们可以把最小二乘拟合向量
  $$
  X\hat\beta^{ls}=X(X^TX)^{-1}X^Ty
  \\=UDV^T(VD^TU^TUDV^T)^{-1}VD^TU^Ty
  \\=UDV^T(VDDV^T)^{-1}VDU^Ty
  \\=U(D^{-1}V^TVDDV^TVD^{-1})^{-1}U^Ty
  \\=UU^Ty
  $$

* 这与满秩分解$QQ^Ty$十分相似，其实$Q$和$U$都是$X$列空间两个不同的正交基（请参考[习题Ex 3.8](A 习题)）

* 得到岭回归的解为
  $$
  X\hat\beta^{ridge}=X(X^TX+\lambda I)^{-1}X^T y
  \\ = UDV^T(VDDV^T+\lambda I)^{-1}VDU^Ty
  \\ =UD(V^TVDDV^TV+\lambda V^TV)^{-1}DU^Ty
  \\ =UD(D^2+\lambda I)^{-1}DU^T y
  \\ = \sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}\mathbf{u_j}\mathbf{u_j}^T y
  $$
  其中$\mathbf{u_j}​$是$U​$的列向量，因为$\frac{d_j^2}{d_j^2+\lambda} < 1​$, 所以上式的含义为

  * 首先算出$y$在$U$上的坐标，然后使用$\frac{d_j^2}{d_j^2+\lambda}$进行收缩基向量的坐标
  * $d_j$越小，收缩基向量的坐标的程度越高

### 2.4 岭回归与特征值分解

* 对于中心化后$X$，样本协方差矩阵是
  $$
  S=X^TX/N
  $$

* 而对$X$ 带入奇异值分解可以得到$X^TX$的特征值分解，特征向量$v_j$($V$的列向量)是$X$的主成分方向
  $$
  X^TX=VD^2V^T
  $$

* 对第一主成分方向$v_1$, 有$z_1=Xv_1$在$X$的列线性组合中方差最大（参考[主成分分析](A 基础知识\A.2 数值方法\4 主成分分析)）， 且方差为
  $$
  Var(z_1)=Var(Xv_1)\\
  =\frac{(Xv_1)^T(Xv_1)}{N}=\frac{v_1^TX^TXv_1}{N}\\
  =\frac{\lambda v_1^Tv}{N}=\frac{d_1^2}{N}
  $$

* 事实上，由于奇异值分解（请参考[奇异值分解](..\A 基础知识\A.1 代数理论\2 协方差矩阵与矩阵分解)），有
  $$
  z_1=Xv_1=u_1d_1
  $$
  * 因此 $u_1$是标准化的第一主成分．后面的主成分 $z_j$ 在与前一个保持正交的前提下有最大的方差 $\frac{d_j^2}{N}$, 因此最后一个主成分有最小的方差。

    证明：
    $$
    <z_i,z_j>=z_i^Tz_j=v_i^TX^TXv_j=\lambda_j v_i^Tv_j=0
    $$
    

  

  * 越小的奇异值 $d_j$ 对应 $X$ 列空间中方差越小的方向，并且岭回归在这些方向上收缩得最厉害
    * ![1615894580524](assets/1615894580524.png)
    * 奇异值越小，对应的$z_j=d_ju_j=Xv_j$的方差越小,
    * 其中$u_i$就相当于经过$v_i$正交基旋转后的属性，这个属性之间也是相互正交的
    * 求$\hat y$时只需要求$y$在$U$列空间的投影，也就是$\hat y=UU^Ty$, 但是岭回归的公式是$\hat y= \sum_{j=1}^p \frac{d_j^2}{d_j^2+\lambda}\mathbf{u_j}\mathbf{u_j}^T y$, 也就是说，岭回归在方差较小的属性$u_i$(X在也是$v_i$ 方向的投影)的系数进行了缩放，而且对低方差成分的系数比高方差收缩得更厉害

### 2.5 自由度

* 岭回归的**有效自由度 (effective degrees of freedom)**定义为
  $$
  df(\lambda) =tr[X(X^TX+\lambda I)^{-1}X^T]
  \\ = tr(H_{\lambda})=\sum_{j=1}^p\frac{d_j^2}{d_j^2+\lambda}
  $$

* 其关于$\lambda$ 单调递减，若$\lambda \rightarrow 0, df(\lambda)=p$, 若$\lambda \rightarrow \infin, df(\lambda) =0​$

* 对截距还有一个额外的自由度，但是事先以及通过中心化去掉了

* 这里自由度的定义，其实可以理解为对$U$列向量投影系数的缩放程度，自由度越低，说明缩放程度越大，对各个属性的约束也越大，概念与子集选择方法系非0变量个数类似

* 若输入变量$X$列正交，则

* 当惩罚参数 $\lambda$ 不同时，前列腺癌例子岭回归的变化曲线

  * 曲线

    ![1615912291307](assets/1615912291307.png)

  * 可以看到随着自由度减少，大部分系数是呈现趋近于0的趋势

  * 但是有部分系数如$lcp,age,gleason​$等等，先趋近于0，然后变为正，然后再趋近于0也是完全可以解释的

    * $lcp$可能是有$v_i+v_j$(两种属性的线性组合)
    * $u_i$的系数可能是正的，$u_j$是系数负的且绝对值更大，自由度是8是$lcp$的系数是负的
    * 随着自由度减少，由于$u_j$的方差更小，收缩更快，导致$lcp$系数变正
    * 当自由度为0时，$lcp$系数变变为0

* 当输入变量$X$列正交时，岭回归估计仅仅是最小二乘估计的缩小版本，也就是
  $$
  \hat\beta^{ridge}=\frac{\hat\beta}{1+\lambda}
  $$
  

## 3 Lasso

### 3.1 概念与公式

* lasso 像岭回归一样是个收缩方法，有微妙但很重要的区别

* 公式

  * 估计定义
    $$
    Q_{t}: \overline \beta^{lasso}=\arg\min_{\beta} \sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p \beta_j x_{ij} )^2\\
    subject\ to \sum_{i=1}^p |\beta_j|\le t
    $$

  * 等价拉格朗日形式
    $$
    P_{\lambda}:\hat\beta^{lasso}=\arg\min_{\beta}\left\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j+\lambda\sum_{j=1}^p |\beta_j|)\right\},\ \lambda \ge 0
    $$
    

    

* 如在岭回归中一样，我们可以通过标准化预测变量来对常数 $\beta_0$ 再参量化,$\beta_0 $ 的解为 $\bar y$，并且后面拟合相对于无截距的模型（见[习题Ex 3.5](A 习题)）

### 3.2 标准化参数

* 定义标准化参数 $s$
  $$
  s = \frac{t}{\sum_{j=1}^p |\beta_j|}
  $$

  * 若$s\ge 1$, 则$\hat\beta^{lasso}=\hat\beta^{ls}$
  * 若$s<1$, 比如$s=0.5$, 最小二乘系数平均收缩 50%,充分小会造成一些参数恰恰等于 0(类似于子集选择)
  * 应该自适应地选择 $t$ 使预测误差期望值的估计最小化

* 曲线

  * ![1615917106126](assets/1615917106126.png)
  * 上图现实了前列腺癌例子中，$s$取不同时的 lasso 系数
  * 该下降不总是严格单调的，尽管例子中确实是
  * lasso 曲线会达到 0，然而岭回归不会．曲线是分段线性的

## 4 子集的选择，岭回归，Lasso

### 4.1 输入变量X正交的情况



## 5 Reference

* 关于正则化，可以参考[正则化](..\B 常用技巧\B.2 正则化与归一化\1 正则化)