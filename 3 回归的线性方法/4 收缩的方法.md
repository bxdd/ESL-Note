# 4 收缩的方法

## 1 概述

* **子集选择 (subset selection)** 可得到一个可解释的、预测误差可能比全模型低的模型。然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差。
* **收缩方法 (shrinkage methods)** 更加连续，因此不会受 **高易变性 (high variability)** 太大的影响．

## 2 岭回归

* 概念：**岭回归 (Ridge regression)** 根据回归系数的大小加上惩罚因子对它们进行收缩。岭回归的系数使得带惩罚的残差平方和最小

* 公式：
  $$
  \hat\beta^{ridge}=\arg\min_{\beta}\left\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j+\lambda\sum_{j=1}^p \beta_j^2)\right\},\ \lambda > 0
  $$
  $\lambda$ 值越大，收缩的程度越大．每个系数都向零收缩。通过参数的平方和来惩罚的想法也用在了神经网络，也被称作**权重衰减 (weight decay)**

* 