# 4 收缩的方法

## 1 概述

* **子集选择 (subset selection)** 可得到一个可解释的、预测误差可能比全模型低的模型。然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差。
* **收缩方法 (shrinkage methods)** 更加连续，因此不会受 **高易变性 (high variability)** 太大的影响．

## 2 岭回归

* 概念：**岭回归 (Ridge regression)** 根据回归系数的大小加上惩罚因子对它们进行收缩。岭回归的系数使得带惩罚的残差平方和最小

* 公式：
  $$
  P_{\lambda}:\hat\beta^{ridge}=\arg\min_{\beta}\left\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j+\lambda\sum_{j=1}^p \beta_j^2)\right\},\ \lambda \ge 0
  $$
  $\lambda$ 值越大，收缩的程度越大．每个系数都向零收缩。通过参数的平方和来惩罚的想法也用在了神经网络，也被称作**权重衰减 (weight decay)**

* 拉格朗日等价：

  岭回归问题可以等价地写成，而且$\lambda$和$t$有一一对应的关系

  * 公式
    $$
    Q_{t}: \overline \beta^{ridge}=\arg\min_{\beta} \sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p \beta_j x_{ij} )^2\\
    subject\ to \sum_{i=1}^p \beta_j^2\le t
    $$

  * 等价的定义

    * 正推：对$\forall \lambda\ge 0​$, 都$\exists t \ge 0​$, 使得$\hat\beta^{ridge}=\overline \beta^{ridge}​$  
    * 反推：对$\forall t \ge 0$, 都$\exists\lambda \ge 0$, 使得$\overline \beta^{ridge}=\hat\beta^{ridge}$ 

  * 证明(为了简化，这里假设不存在$\beta_0​$)：

    * 对于$P_{\lambda}​$，有
      $$
      \frac{\part (y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta}{\part \beta} =0
      \\ \leftrightarrow X^T(X\beta - y) + \lambda \beta= 0
      $$

    * 对于$Q_{t}​$

      * 首先可以化为矩阵形式
        $$
        Q_{t}: \overline \beta^{ridge}=\arg\min_{\beta} (y-X\beta)^T(y-X\beta)\\
        subject\ to\ \beta^T\beta-t\le 0
        $$

      * 由拉格朗日函数法
        $$
        L(\beta,\mu)=(y-X\beta)^T(y-X\beta)+\mu(\beta^T\beta-t)
        $$

      * 根据$KTT​$条件
        $$
        \left\{
        \begin{align}
        &\nabla f + \mu \nabla g = 0\\
        &\ g(x) \le 0\\
        &\mu >= 0\\
        &\mu g(x) = 0
        &\end{align}
        \right.
        $$
        有
        $$
        \leftrightarrow \\
        \left\{
        \begin{align}
        &X^T(X\beta - y) + \mu \beta=0\\
        &\ \beta^T\beta-t\le 0\\
        &\mu >= 0\\
        &\mu(\beta^T\beta-t) = 0
        &\end{align}
        \right.
        $$
        

    * 首先证明正推：

      * 对$\forall \lambda\ge 0$，可得$P_{\lambda}$ 的解为
        $$
        \hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty
        $$

      * 只需要使得$Q_{t}$中
        $$
        \mu=\lambda\\
        t=((X^TX+\lambda I)^{-1}X^Ty)^T(X^TX+\lambda I)^{-1}X^Ty
        $$

      * 即可使得两者解相同

    * 然后证明反推：

      * 对$\forall t \ge 0 ​$， 取$\mu​$ 是满足$KTT​$ 条件的一个值

        * 若$t\ge ((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty, 此时取​$$\mu =0 ​$即可满足$KTT​$条件，则也取$\lambda =0​$ 即可

        * 若$t< ((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty​$, 则必有
          $$
          \mu !=0\\
          t=\beta^T\beta
          $$
          取$\mu$取满足下式的一个值即可
          $$
          ((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty=t\\
          \label{eq2}
          $$

          * 因为

            $$
            (1)\lim_{\mu\rightarrow\infin}((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty
            \\=((\mu I)^{-1}X^Ty)^T(\mu I)^{-1}X^Ty=0<t\\
            (2)when_{\mu=0}((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty
            \\=((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty>t
            $$
          * 由于函数连续性，所以则必定$\exists \mu$, 满足式子 $\eqref{eq2}$

          此时也取$\lambda=\mu$即可