# 4 收缩的方法

## 1 概述

* **子集选择 (subset selection)** 可得到一个可解释的、预测误差可能比全模型低的模型。然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差。
* **收缩方法 (shrinkage methods)** 更加连续，因此不会受 **高易变性 (high variability)** 太大的影响．

## 2 岭回归

### 2.1 概念与公式

* 概念：**岭回归 (Ridge regression)** 根据回归系数的大小加上惩罚因子对它们进行收缩。岭回归的系数使得带惩罚的残差平方和最小

* 公式：
  $$
  P_{\lambda}:\hat\beta^{ridge}=\arg\min_{\beta}\left\{\sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p x_{ij}\beta_j+\lambda\sum_{j=1}^p \beta_j^2)\right\},\ \lambda \ge 0
  $$
  $\lambda$ 值越大，收缩的程度越大．每个系数都向零收缩。通过参数的平方和来惩罚的想法也用在了神经网络，也被称作**权重衰减 (weight decay)**

* 拉格朗日等价：

  岭回归问题可以等价地写成，而且$\lambda$和$t$有一一对应的关系

  * 公式
    $$
    Q_{t}: \overline \beta^{ridge}=\arg\min_{\beta} \sum_{i=1}^N (y_i-\beta_0-\sum_{j=1}^p \beta_j x_{ij} )^2\\
    subject\ to \sum_{i=1}^p \beta_j^2\le t
    $$

  * 等价的定义

    * 正推：对$\forall \lambda\ge 0​$, 都$\exists t \ge 0​$, 使得$\hat\beta^{ridge}=\overline \beta^{ridge}​$  
    * 反推：对$\forall t \ge 0$, 都$\exists\lambda \ge 0$, 使得$\overline \beta^{ridge}=\hat\beta^{ridge}$ 

  * 证明(为了简化，这里假设不存在$\beta_0$, 即无截距)：

    * 对于$P_{\lambda}​$，有
      $$
      \frac{\part (y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta}{\part \beta} =0
      \\ \leftrightarrow X^T(X\beta - y) + \lambda \beta= 0
      $$

    * 对于$Q_{t}​$

      * 首先可以化为矩阵形式
        $$
        Q_{t}: \overline \beta^{ridge}=\arg\min_{\beta} (y-X\beta)^T(y-X\beta)\\
        subject\ to\ \beta^T\beta-t\le 0
        $$

      * 由拉格朗日函数法
        $$
        L(\beta,\mu)=(y-X\beta)^T(y-X\beta)+\mu(\beta^T\beta-t)
        $$

      * 根据$KTT​$条件
        $$
        \left\{
        \begin{align}
        &\nabla f + \mu \nabla g = 0\\
        &\ g(x) \le 0\\
        &\mu >= 0\\
        &\mu g(x) = 0
        &\end{align}
        \right.
        $$
        有
        $$
        \left\{
        \begin{align}
        &X^T(X\beta - y) + \mu \beta=0\\
        &\ \beta^T\beta-t\le 0\\
        &\mu >= 0\\
        &\mu(\beta^T\beta-t) = 0
        &\end{align}
        \right.
        $$

    * 首先证明正推：

      * 对$\forall \lambda\ge 0$，可得$P_{\lambda}$ 的解为
        $$
        \hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty
        $$

      * 只需要使得$Q_{t}$中
        $$
        \mu=\lambda\\
        t=((X^TX+\lambda I)^{-1}X^Ty)^T(X^TX+\lambda I)^{-1}X^Ty
        $$

      * 即可使得两者解相同

    * 然后证明反推：

      * 对$\forall t \ge 0 ​$， 取$\mu​$ 是满足$KTT​$ 条件的一个值

        * 若$t\ge ((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty, 此时取​$$\mu =0 ​$即可满足$KTT​$条件，则也取$\lambda =0​$ 即可

        * 若$t< ((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty​$, 则必有
          $$
          \mu !=0\\
          t=\beta^T\beta
          $$
          取$\mu$取满足下式的一个值即可
          $$
          ((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty=t\\
          \label{eq2}
          $$

          * 因为

            $$
            (1)\lim_{\mu\rightarrow\infin}((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty
            \\=((\mu I)^{-1}X^Ty)^T(\mu I)^{-1}X^Ty=0<t\\
            (2)when_{\mu=0}((X^TX+\mu I)^{-1}X^Ty)^T(X^TX+\mu I)^{-1}X^Ty
            \\=((X^TX)^{-1}X^Ty)^T(X^TX)^{-1}X^Ty>t
            $$
          * 由于函数连续性，所以则必定$\exists \mu$, 满足式子 $\eqref{eq2}$

          此时也取$\lambda=\mu$即可

* 为什么这么做：当在线性回归模型中有许多相关变量，它们的系数可能很难确定且有高方差．某个变量的较大的正系数可以与相关性强的变量的差不多大的负系数相互抵消。通过对系数加入大小限制，问题可以减轻

* 矩阵形式

  * 输入标准化：

    * 由于对输入按比例进行缩放时，岭回归的解不相等，因此求解前我们需要对输入进行标准化。

    * 注意到惩罚项不包含截距 $\beta_0$，因为对截距的惩罚会使得过程依赖于 $y$ 的初始选择（每个 $y_i$ 加上常数 $c$ 不是简单地导致预测值会偏离同样的量 $c​$）

    * 可以证明，当对输入进行中心化后（即$\mathbf{x}_{norm}=\mathbf{x}-\bar{\mathbf{x}}​$)，$Q_t​$的解可以分为两部分

      * 使用$\bar y=\frac{\sum_{i=1}^N y_i}{N}$来估计$\beta_0$
      * 使用无截距的岭回归，利用中心化的$\mathbf{x}$估计剩余部分参数

      

    证明请看[习题Ex 3.5](A 习题)

  * 公式
    $$
    RSS(\lambda)=(y-X\beta)^T(y-X\beta)+\lambda\beta^T\beta
    $$
    因此，岭回归可以写作如上，其中$X$是$p$列，每一行都是经过中心化了的输入变量。而$y$也是传统岭回归的$y_{norm}=y-\beta_0=y-\bar y$中心化的值

    解：
    $$
    \frac{\part (y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta}{\part \beta} =0
    \\ \leftrightarrow X^T(X\beta - y) + \lambda \beta= 0
    \\ \leftrightarrow \hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty
    $$

  * 由岭回归矩阵形式的解$\hat\beta^{ridge}=(X^TX+\lambda I)^{-1}X^Ty$可以看到，即使$X^TX$不是满秩的，$X^TX+\lambda I$也是满秩矩阵，这样会使得问题非奇异，而且这是第一次将岭回归引入统计学中的主要动力

    * 证明：
      $$
      \forall x >0, x^T(X^TX+\lambda I)x=(Xx)^TXx+\lambda x^Tx > 0
      $$
      因此$X^TX+\lambda I​$正定，正定矩阵必定满秩，证毕

### 2.2 岭回归与后验分布

* 当给定一个合适的先验分布，岭回归也可以从后验分布的均值或众数得到

* 假设

  * $y$的先验分布为 $N(\beta_0+X\beta,\sigma^2I)$
  * 参数 $\beta$ 的前验分布为 $N(0,\tau^2I)$
  * 参数$\beta_0$不包含任何前验信息，即前沿分布是均匀分布

* 则$\beta $ 后验分布密度函数的对数值就是$ridge$

* 证明

  

  * 由后验概率公式，可得
    $$
    P([\beta_0,\beta]|y) = \frac{P([\beta_0,\beta],y)}{P(y)} = \frac{P(y|[\beta_0,\beta])P([\beta_0,\beta])}{P(y)}  \\\sim P(y|[\beta_0,\beta])P(\beta)P(\beta_0) \sim P(y|[\beta_0,\beta])P(\beta) 
    \\=\frac{1}{\sqrt{(2\pi\sigma^2)^{N}}}\exp(-\frac{(y-\beta_0-X\beta)^T(y-\beta_0-X\beta)}{2\sigma^2})\times \frac{1}{\sqrt{(2\pi\tau)^{N}}}\exp(-\frac{\beta^T\beta}{2\tau^2})\\
    =C\exp(-\frac{1}{2\sigma^2}((y-\beta_0-X\beta)^T(y-\beta_0-X\beta)+\frac{\sigma^2}{\tau^2}\beta^T\beta))
    $$

  * 取负对数
    $$
    -\log P([\beta_0,\beta]|y)=-\log C+\frac{1}{2\sigma^2}((y-\beta_0-X\beta)^T(y-\beta_0-X\beta)+\frac{\sigma^2}{\tau^2}\beta^T\beta)
    $$

  * 因为后验分布仍然是高斯分布，因此是众数也是均值

### 2.3 岭回归与奇异值分解

* 中心化输入矩阵$X_{N\times p}​$进行**奇异值分解 (SVD)** 可以进一步了解了岭回归
  $$
  X_{N\times p}=U_{N\times p}D_{p\times p}V_{p\times p}^T
  $$

  * 这是对标准奇异值分解进行矩阵分块得到的，称为Thin SVD（请参考[奇异值分解](A 基础知识\A.1 代数理论\2 协方差矩阵与矩阵分解))

  * $D_{p\times p}$是对角阵，其对角元$d_1\ge d_2\ge \dots \ge d_p \ge 0$, 若有$d_j=0$，则称$X$为奇异的。其实若$X$列满秩，则$D​$也是非奇异对角阵

    

* 利用奇异值分解，通过简化我们可以把最小二乘拟合向量
  $$
  X\hat\beta^{ls}=X(X^TX)^{-1}X^Ty
  \\=UDV^T(VD^TU^TUDV^T)^{-1}VD^TU^Ty
  \\=UDV^T(VDDV^T)^{-1}VDU^Ty
  \\=U(D^{-1}V^TVDDV^TVD^{-1})^{-1}U^Ty
  \\=UU^Ty
  $$

* 这与满秩分解$QQ^Ty$十分相似，其实$Q$和$U$都是$X$列空间两个不同的正交基

  