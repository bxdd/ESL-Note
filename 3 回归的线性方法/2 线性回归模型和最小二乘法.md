# 线性回归模型和最小二乘法

## 1 线性回归模型

* 形式：
  $$
  f(x)=x^T\beta=\beta_0+\sum_{j=1}^p x_j\beta_j
  $$

* 变量 $x_j​$ 可以有下列不同的来源(无论 $x_j​$ 是哪个来源，模型关于参数都是线性的)

  * 定量的输入变量
  * 定量输入变量的变换，比如对数变换，平方根变换或者平方变换
  * 基函数展开，比如$(x_1,x_2=x_1^2,...x_N=x_1^N)​$
  * 定性输入变量水平 (level) 的数值或“虚拟”编码
    * 比如$G​$有5种水平，若$G=j​$，可以构造$(x_1=0,...,x_j=1,...,x_N=0)​$
  * 变量的交叉影响，比如$(x_1,x_2,x_3=x_1x_2)$

* 什么是线性模型

  * 统计意义：若一个回归等式是线性的，其参数就必须也是线性的。对于参数是线性，即使样本变量的特征(属性)是二次方或者多次方，这个回归模型也是线性的。

  * 线性和非线性的区别是是否可以用直线将样本划分开：线性模型可以是用曲线拟合样本，但是分类的决策边界一定是直线的，例如logistics模型。

    * $$
      y=\frac{1}{1+e^{x^T\beta}}\\
      y_0=\frac{1}{1+e^{x^T\beta}}\ (y_0是决策点)\\
      x^T\beta=\ln(\frac{1}{y_0}-1)\ (可以看出决策边界线性)\\
      $$

  * 如何区分：

    * 若每个参数$\beta_j$都只影响一个$x_j$，则是线性的；否则是非线性
    * 例如
      * 线性：$\frac{1}{1+e^{x^T\beta}}$
      * 非线性：$\frac{1}{1+\beta_1e^{x^T\beta}} $

## 2 最小二乘法

### 2.1 公式推导

* 残差平方和
  $$
  RSS(\beta)=(y-X\beta)^T(y-X\beta)
  $$

* 求解
  $$
  \frac{\partial RSS(\beta)}{\partial \beta} = 2X^T(X\beta - y) = 0\\
  \hat \beta=(X^TX)^{-1}X^Ty\\ \label{eq1}
  \hat y =X\hat\beta=X(X^TX)^{-1}X^Ty
  $$
  

### 2.2 最小二乘估计的几何表示

* 记 $X​$ 的列向量为 $x_0,x_1,…,x_p​$，其中 $x_0≡1​$。这些向量张成了 $R^N​$ 的子空间，也被称作 $X​$ 的列空间$Col(X)​$。

  * 当$X​$列满秩，$X^TX​$可逆(证明见[子空间与投影矩阵](..\A 基础知识\A.1 代数\3 子空间与投影矩阵))，则由公式 $\ref{eq1}​$, 可以看出$(\hat y-y)=(X\hat \beta-y)​$与$X​$的列向量均正交

    * 由于$\hat y=X\beta=\sum_{i=1}^px_i\beta_i \in Col(X)$，所以$\hat y$是$y​$在子空间中的投影
    * 同时也可以得出投影$\hat y ​$是到$y​$欧氏距离最近的点

    ![1613929423283](assets/1613929423283.png)

    * 此时，$\hat y = X\hat\beta=(X^TX)^{-1}X^Ty$，则称帽子矩阵$H=(X^TX)^{-1}X^T$，同时他也是$y$到$X$子空间的投影矩阵

  * 当$X$的并不是列满秩时，$X^TX$是奇异的，因此$\hat \beta$不唯一

    * 但是同理，$X\hat\beta ​$仍旧是$y​$在子空间中的投影
    * 只不过，用$X$ 的列向量来表示这种投射的方式不止一种，但这并不代表投影有多个
    * 当一个或多个定性输入用一种冗余的方式编码时经常出现非满秩的情形．通过重编码或去除 $X$ 中冗余的列等方式可以解决非唯一表示的问题

### 2.3 参数$\hat \beta$的显著性检验

* 什么是显著性检验：

  * 显著性，又称统计显著性（Statistical significance）， 是指零假设为真的情况下拒绝零假设所要承担的风险水平，又叫概率水平

  * 显著性水平：在原假设是真实的情况下，犯错的概率是一个固定的值，称为显著性水平$\alpha$, 而$1-\alpha$又叫置信水平

    ![1614098759349](assets/1614098759349.png)

  * $p-value$：发生某个时间的概率，一般认为$p-values < \alpha$就是小概率发生事件，就可以拒绝原建设

* 一些假设：

  * 为了约束 $\beta$ 的取样特点，我们现在假设观测值$y_i$ 不相关，且有固定的方差 $\sigma^2$，并且 $x_i$ 是固定的（非随机）

  * 假设$E(Y|X)$是线性的，因此$f(x)=x^T\beta$是正确的模型

  * 假设 $Y$ 与其期望的偏差是可加的且是正态的
    $$
    y=E(Y|X)+\epsilon=x^T\beta+\epsilon
    $$
    其中$\epsilon \sim N(0,\sigma^2)$



* $\sigma^2​$的无偏估计和分布

  * 公式：
    $$
    \hat\sigma^2=\frac{1}{N-p-1}\sum_{i=1}^N(y_i-\hat y_i)^2
    $$

  * 无偏性：$N-p-1$使得估计是无偏估计，即$E(\hat\sigma^2)=\sigma^2$

  * 证明：

    * 首先化简 

      $$
      \hat y = X\hat \beta =X(X^TX)^{-1}X^Ty\\
      \sum_{i=1}^N(y_i-\hat y_i)^2 = (\hat y-y)^T(\hat y-y)\\
      = (X(X^TX)^{-1}X^T(X\beta+\epsilon)-(X\beta+\epsilon))^T(X(X^TX)^{-1}X^T(X\beta+\epsilon)-(X\beta+\epsilon))
      \\ = (X\beta+X(X^TX)^{-1}X^T\epsilon-X\beta-\epsilon)^T(X\beta+X(X^TX)^{-1}X^T\epsilon-X\beta-\epsilon)
      \\ = \epsilon^T(X(X^TX)^{-1}X^T-I)^T(X(X^TX)^{-1}X^T-I)\epsilon
      \\ = \epsilon^T(X(X^TX)^{-1}X^T-2X(X^TX)^{-1}X^T+I)\epsilon
      \\ = \epsilon^T(I-X(X^TX)^{-1}X^T)\epsilon
      \\ = \epsilon^T(I-H)\epsilon
      $$

    * 定理：列满秩左乘不改变秩，行满秩右乘不改变秩(设$X$列满秩，$Y$行满秩)
      $$
      若XAx=0,则X^TXAx=0,则Ax=0\\
      若Ax=0，则XAx=0\\
      则XAx=0和Ax=0的解集一样，则Rank(XA)=Rank(A)
      $$

    * 因此
      $$
      \\ Rank(H=X(X^TX)^{-1}X^T)=Rank((X^TX)^{-1})=p+1
      $$

    * 由于
      $$
      R(A)+R(B)\le n+R(AB)\\
      R(A)+R(B)\ge R(A+B)
      $$

    * 定理：若$A^2=A$, 就有$R(I-A)+R(A)=n$
      $$
      R(A)+R(I-A)\le n +R(A(I-A)=A-A=0)\\
      R(A)+R(I-A)\ge R(A+I-A)=n\\
      R(A)+R(I-A)=n\\
      R(I-A)=n-R(A)=N-p-1
      $$

    * 由于$H=X(X^TX)^{-1}X^T, H^2=H$， 所以$R(I-H)=n-R(H)=N-p-1$

    * 定理：若$A^2=A$, 则$A$的特征值一定是0或者1
      $$
      若A!=0\&\&A!=I,则A^2-A=0\ 是最小多项式\\
      由于A的特征多项式和最小多项式根相同（重数可能不一样）\\
      所以A的特征值只能是0或者1
      $$

    * 由于$(I-H)^2=I-2H+H^2=I-H$是，所以$I-H$的特征值只能是0或者1

    * 又因为$I-H$对称，可以进行特征值分解
      $$
      I-H=U^T\Lambda U\ (U是正交矩阵，\Lambda 是特征值从大到小排列对角阵)
      \\ \because Rank(I-H)=Rank(\Lambda)=N-p-1\ (可逆变换不改变秩)
      \\ \Lambda 对角线前(N-p-1)元素是1，其余是0
      $$

    * 因此
      $$
      \epsilon^T(I-H)\epsilon=(U\epsilon)^T\Lambda U\epsilon 
      $$

    

    * 由于正态分布可加性，$U\epsilon​$的每一行都符合正态分布，且和$\epsilon​$的分布都为$N(0,\sigma^2)​$
      $$
      U\epsilon=(p_1,p_2,...,p_N)^T\\
      E(p_i)=0\\
      Var(p_i)=U_{i,:}\epsilon=\sigma^2 \\
      $$

    * 定理：独立同分布的正态变量经过正交变换仍保持独立

      由于正态变量独立和不相关是等价的，所以只需要证明$Cov(U_{i,:}^T\epsilon,U_{j,:}^T\epsilon)=0$
      $$
      Cov(U_{i,:}^T\epsilon,U_{j,:}^T\epsilon)
      =E(U_{i,:}^T\epsilon U_{j,:}^T\epsilon)-E(U_{i,:}^T\epsilon)E(U_{j,:}^T\epsilon)
      
      \\ =E(\sum_{k=1}^{N}\sum_{l=1}^NU_{ik}U_{jl}\epsilon_{k}\epsilon_{j})-\sum_{k=1}^N E(U_{ik}\epsilon_k)\sum_{l=1}^N E(U_{jl}\epsilon_l)
      \\ = \sum_{k=1}^{N}\sum_{l=1}^NE( U_{ik}U_{jl}\epsilon_{k}\epsilon_{j})-\sum_{k=1}^N\sum_{l=1}^N E(U_{ik}\epsilon_k) E(U_{jl}\epsilon_l)
      \\ = \sum_{k=1}^N\sum_{l=1}^N E(U_{ik}\epsilon_k) E(U_{jl}\epsilon_l)-\sum_{k=1}^N\sum_{l=1}^N E(U_{ik}\epsilon_k) E(U_{jl}\epsilon_l) = 0
      $$

    * 因此，$p_i$互相独立，且$p_i\sim N(0,\sigma^2)$
      $$
      (U\epsilon)^T\Lambda U\epsilon =\sum_{i=1}^{N-p-1} (U_{i,:}\epsilon_i)^2=\sum_{i=1}^{N-p-1} p_i^2
      $$

    * 因此
      $$
      \hat\sigma^2=\frac{1}{N-p-1}\sum_{i=1}^N(y_i-\hat y_i)^2=\frac{\sum_{i=1}^{N-p-1} p_i^2}{N-p-1}\\
      (N-p-1)\frac{\hat\sigma^2}{\sigma^2} \sim \chi_{N-p-1}\\
      E((N-p-1)\frac{\hat\sigma^2}{\sigma^2})= N-p-1\\
      E(\hat \sigma^2) = \sigma^2
      $$

    

* $\hat \beta$分布、期望与协方差

  * 公式
    $$
    \hat \beta \sim N(\beta, (X^TX)^{-1}\sigma^2)
    $$

  * 证明（TODO）：

    * 根据最小二乘公式
      $$
      \hat \beta =(X^TX)^{-1}X^T(X\beta + \mathbf{\epsilon})\\
      \epsilon=(\epsilon_1,\epsilon_2,...,\epsilon_N)^T
      $$

    * 定理1(目前不对)：若$y=Ax+b$, 且$A$列满秩，则$p_Y(Y=y) = (A^TA)^{-1}Ap_X(X=(A^TA)^{-1}A^T(y-b))$

      证明：
      $$
      f_Y(Y=Ax+b)=f_X(X=x)\\
      \frac{\part Y}{\part x}p_Y(Y=Ax+b)=p_X(X=x)\ (两侧同时求导，具体可见 链式法则) \\
      A^Tp_Y(Y=Ax+b)=p_X(X=x)\\
      A^TAp_Y(Y=Ax+b)=Ap_X(X=x)\ (A列满秩则A^TA可逆)\\
      p_Y(Y=Ax+b) = (A^TA)^{-1}Ap_X(X=x)\\
      x=(A^TA)^{-1}A^T(y-b) \\
      p_Y(Y=y) = (A^TA)^{-1}Ap_X(X=(A^TA)^{-1}A^T(y-b))\\
      $$

    * 定理2(目前不对)：$y$是$x$的仿射变换($y = Ax+b$且$A$列满秩)，那$\hat y$也服从多元正态分布,

      * $x=(A^TA)^{-1}A^T(y-B)​$

      * 证明
        $$
        p_X(x)=\frac{1}{\sqrt{(2\pi)^{k}|\Sigma|}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\\
        
        p_Y(y)=(A^TA)^{-1}Ap_X((A^TA)^{-1}A^T(y-b))\ (定理1)
        \\ = (A^TA)^{-1}A\frac{1}{\sqrt{(2\pi)^{k}|\Sigma|}}\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\\
        $$

      

    

    * $\hat \beta$的期望和协方差矩阵为
      * 期望
        $$
      E(\hat\beta)=(X^TX)^{-1}X^TX\beta=\beta\\
        $$
      * 协方差
        $$
      Cov(\hat\beta)=E((\hat \beta-E(\hat \beta)(\hat \beta-E(\hat \beta)^T)\\=((X^TX)^{-1}X^T(X\beta + \mathbf{\epsilon})-E((X^TX)^{-1}X^T(X\beta + \mathbf{\epsilon})))^T((X^TX)^{-1}X^T(X\beta + \mathbf{\epsilon})-E((X^TX)^{-1}X^T(X\beta + \mathbf{\epsilon})))
      \\=E((X^TX)^{-1}X^T\mathbf{\epsilon})^TE((X^TX)^{-1}X^T\mathbf{\epsilon})
      \\=(X^TX)^{-1}X^T\sigma^2IX(X^TX)^{-1}
      \\ = (X^TX)^{-1}\sigma^2
        $$
      所以$\hat \beta \sim N(\beta, (X^TX)^{-1}\sigma^2)​$

      * PS：目前还未证明其符合正态分布


* 显著性检验

  * 单系数显著性检验

    * 变量：

      * $v_j$:$(X^TX)^{-1}$第$j$个对角元
      * $\hat\beta_j$:$\hat\beta $的第$j$个元素

    * 零假设：假设$\hat\beta_j = 0$

    * t统计量检验
      $$
      \frac{\hat\beta_j}{\sqrt{v_j}\sigma} \sim N(0,1)\\
      (N-p-1)\frac{\hat\sigma^2}{\sigma^2} \sim \chi_{N-p-1}\\
      \frac{\frac{\hat\beta_j}{\sqrt{v_j}\sigma}}{\sqrt{\frac{\hat\sigma^2}{\sigma^2}}}=\frac{\hat\beta_j}{\sqrt{v_j}\hat\sigma} \sim t(N-p-1)
      $$

    * 正态统计量检验
      $$
      \frac{\hat\beta_j}{\sqrt{v_j}\sigma} \sim N(0,1)\\
      $$
      

  * 多系数显著性检验

    * 情景：检验有 $k$ 个水平的分类变量是否要从模型中排除，我们需要检验用来表示水平的虚拟变量的系数是否可以全部设为 0

    * 假设：有$k$个变量均为0

    * F检验统计量
      $$
      F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)}
      $$

    *  $RSS1$ 是有 $p_1+1$ 个参数的大模型的最小二乘法拟合的残差平方和，$RSS_0$ 是有 $p_0+1$ 参数的小模型的最小二乘法拟合的残差平方和，其有 $p_1−p_0$ 个参数约束为 0

    * 

