# 线性回归模型和最小二乘法

## 1 线性回归模型

* 形式：
  $$
  f(X)=X\beta=\beta_0+\sum_{j=1}^p X_j\beta_j
  $$

* 变量 $X_j$ 可以有下列不同的来源(无论 $X_j$ 是哪个来源，模型关于参数都是线性的)

  * 定量的输入变量
  * 定量输入变量的变换，比如对数变换，平方根变换或者平方变换
  * 基函数展开，比如$(X_1,X_2=X_1^2,...,X_N=X_1^N)$
  * 定性输入变量水平 (level) 的数值或“虚拟”编码
    * 比如$G$有5种水平，若$G=j$，可以构造$(X_1=0,...,X_j=1,...,X_N=0)$
  * 变量的交叉影响，比如$(X_1,X_2,X_3=X_1X_2)$

* 什么是线性模型

  * 统计意义：若一个回归等式是线性的，其参数就必须也是线性的。对于参数是线性，即使样本变量的特征(属性)是二次方或者多次方，这个回归模型也是线性的。

  * 线性和非线性的区别是是否可以用直线将样本划分开：线性模型可以是用曲线拟合样本，但是分类的决策边界一定是直线的，例如logistics模型。

    * $$
      y=\frac{1}{1+e^{x^T\beta}}\\
      y_0=\frac{1}{1+e^{x^T\beta}}\ (y_0是决策点)\\
      x^T\beta=\ln(\frac{1}{y_0}-1)\ (可以看出决策边界线性)\\
      $$

  * 如何区分：

    * 若每个参数$\beta_j$都只影响一个$x_j$，则是线性的；否则是非线性
    * 例如
      * 线性：$\frac{1}{1+e^{x^T\beta}}$
      * 非线性：$\frac{1}{1+\beta_1e^{x^T\beta}} $

## 2 最小二乘法

### 2.1 公式推导

* 残差平方和
  $$
  RSS(\beta)=(y-X\beta)^T(y-X\beta)
  $$

* 求解
  $$
  \frac{\partial RSS(\beta)}{\partial \beta} = 2X^T(X\beta - y) = 0\\
  \hat \beta=(X^TX)^{-1}X^Ty\\ \label{eq1}
  \hat y =X\hat\beta=X(X^TX)^{-1}X^Ty
  $$
  

### 2.2 最小二乘估计的几何表示

* 记 $X​$ 的列向量为 $x_0,x_1,…,x_p​$，其中 $x_0≡1​$。这些向量张成了 $R^N​$ 的子空间，也被称作 $X​$ 的列空间$Col(X)​$。

  * 当$X​$列满秩，$X^TX​$可逆(证明见[子空间与投影矩阵](..\A 基础知识\A.1 代数\3 子空间与投影矩阵))，则由公式 $\ref{eq1}​$, 可以看出$(\hat y-y)=(X\hat \beta-y)​$与$X​$的列向量均正交

    * 由于$\hat y=X\beta=\sum_{i=1}^px_i\beta_i \in Col(X)$，所以$\hat y$是$y​$在子空间中的投影
    * 同时也可以得出投影$\hat y ​$是到$y​$欧氏距离最近的点

    ![1613929423283](assets/1613929423283.png)

    * 此时，$\hat y = X\hat\beta=(X^TX)^{-1}X^Ty$，则称帽子矩阵$H=(X^TX)^{-1}X^T$，同时他也是$y$到$X$子空间的投影矩阵

  * 当$X$的并不是列满秩时，$X^TX$是奇异的，因此$\hat \beta$不唯一

    * 但是同理，$X\hat\beta ​$仍旧是$y​$在子空间中的投影
    * 只不过，用$X$ 的列向量来表示这种投射的方式不止一种，但这并不代表投影有多个
    * 当一个或多个定性输入用一种冗余的方式编码时经常出现非满秩的情形．通过重编码或去除 $X$ 中冗余的列等方式可以解决非唯一表示的问题

### 2.3 参数$\hat \beta$的性质

* 假设：为了约束 $\beta$ 的取样特点，我们现在假设观测值$y_i$ 不相关，且有固定的方差 $\sigma_2$，并且 $x_i$ 是固定的（非随机）

* $\hat \beta$的协方差矩阵
  $$
  Var(\hat\beta)=(X^TX)^{-1}\sigma^2
  $$
  证明：

  * 由于$\hat \beta=(X^TX)^{-1}X^Ty​$
  * 由于$E(\hat \beta)=(X^TX)^{-1}X^TE(y)=0​$，所以
  * 则$Cov(\hat\beta)=E((\hat \beta-E(\hat \beta)(\hat \beta-E(\hat \beta)^T)=E(\hat \beta\hat \beta^T)​$
  * 从而推出$E(\hat \beta\hat \beta^T)=E((X^TX)^{-1}X^Tyy^TX(X^TX)^{-1})=(X^TX)^{-1}X^TE(yy^T)X(X^TX)^{-1}​$
  * 然后$E(yy^T)=\sigma^2I​$
  * 因此$Cov(\hat\beta)=(X^TX)^{-1}X^T\sigma^2IX(X^TX)^{-1}=\sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1}=(X^TX)^{-1}\sigma^2$