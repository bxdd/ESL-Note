# 子集的选择

## 1  概述

* 不使用最小二乘估计的某些原因
  * 预测的精确性：最小二乘估计经常有小偏差大方差。预测精确性有时可以通过收缩或者令某些系数为 $0$ 来提高
  * 可解释性：当有大量的预测变量时，我们经常去确定一个小的子集来保持最强的影响，可以牺牲一些小的细节

* **模型选择** ：
  * 一些线性回归选择变量子集
  * 于控制方差的收缩和混合的方法，以及其它降维的策略

## 2 选择子集的策略

### 2.1 最优集选择

* 问题描述：对于每个 $k\in\{0,1,2,…,p\}$，最优子集回归要找出规模为 $k$ 的子集中残差平方和 $RSS​$ 最小的子集

* leaps and bounds 过程

  * 子集模型和最优秀子集曲线

    ![1614445139577](assets/1614445139577.png)

    * 下边界代表通过最优子集方法选择的符合条件的模型，也就是最有子集曲线
    * 横轴：子集大小
    * 纵轴：残差平方和$RSS$
    * 较大的子集不一定嵌套较小的子集

  * 选择$k$

    * 最优子集曲线必然地下降，所以不能用来选择子集的规模$k$
    * 怎样选择 $k$ 涉及偏差和方差之间的平衡
    * 有许多可能会使用的准则，例如
      * 选择最小的模型使得预测误差期望值的估计最小
      * 采用交叉验证去估计预测误差并选择 $k$

  * 在$p<40$可行，否则复杂度太高

### 2.2 向前和向后逐步选择

* 向前逐步选择(Forward-stepwise selection) : 从截距开始，然后向模型中依次添加最大程度提升拟合效果的预测变量

  * 仍需要确定$k​$
  * 属于贪心算法 (greedy algorithm)，产生一个嵌套的模型序列，较大的子集一定嵌套较小的子集。
  * 与最优子集选择相比似乎是次优的．然而，有许多原因表明向前逐步可能是更好的：
    * 计算：对于大的$p$（$p>>N​$)，复杂度更小
    * 统计（还未理解）：在每个规模下选择最优子集需要在方差上付出代价；向前逐步是一种有更多约束的搜索，而且将会有更低的方差，但是或许有更大的偏差
  * 有大量的备选预测变量时，会需要大量的计算。然而却可以利用$QR$分解从当前拟合快速得到下一步的备选预测变量（具体见[习题Ex 3.9](A 习题))）

* 向后逐步选择 (backward-stepwise selection)：从整个模型开始，并且逐步删掉对拟合影响最低的预测变量

  * 要删掉的候选变量是 $Z$ 分数最低的变量（具体见[习题Ex 3.10](A 习题))

  * 只能用于$N>p​$的情况

    

* 参数方差比较

  * 这是一个小型模拟研究的结果

    ![1614445934047](assets/1614445934047.png)

    * 问题描述
      * 一个模拟的线性回归问题 $Y=Xβ+\epsilon$ 中四种子集选择方法的比较。在 $p=31$ 个标准高斯变量有 $N=300$ 个观测，且成对相关系数都等于 0.85
      * 有10个变量的系数是从$N(0,0.4)$选取的，其他都是0
      * 噪声$\epsilon \sim N(0,6.25)$，信噪比是0.64
    * 解释
      * 横轴：子集大小
      * 纵轴：$E(\|\hat\beta(k)-\beta\|^2)$，参数的均方误差
      * 生成方法是，首先生成$\beta$，然后多次随机生成300个观测，生成$\hat\beta(k)$。最后根据多次生成的$\hat\beta(k)$计算均方误差
    * 分析
      * 因为只有10个参数不是10，所以只要10个变量有效，可以看到基本上都是在$k=10​$就降低到一个比较小的值
      * 同时，因为$E(\|\hat\beta(k)-\beta\|^2)$可以看作预测值和真实值的均方误差，因此可以分解为偏差+方差。因此这个曲线并不是严格下降，可逆存在上升的情况（偏差增大但是方差减小）

* 其他

  * 传统的包中的选择基于 $F$ 统计量，加入“显著性”的项，然后删掉“非显著性”的项，但是不再流行，因为没有考虑多重检验（TODO：对多重检验还有疑问）
  * 些软件包实现混合的逐步选择策略，在每一步同时考虑向前和向后的移动，然后选择两者中最好的一个
  * 对于变量经常成群出现（比如用来编码多层次类别型预测变量的虚拟变量），智能逐步过程会合理考虑到它们的自由度会一次添加或删除整个群体

### 2.3 向前逐渐 (Forward-Stagewise) 回归

* Forward-Stagewise 回归比向前逐步回归有更多限制

* 过程：开始类似向前逐步回归，由等于 $\bar y$ 的截距开始，中心化后的预测变量系数都初始化为 0。每一步算法找出与当前残差最相关的变量。然后计算所选择变量的残差的简单线性回归系数，并且添加到该变量的当前系数。这个过程一直继续直到没有变量与残差有相关性——比如，当 $N>p​$ 时的最小二乘拟合。

  * 令当前的回归预测是$\hat \mu$，初始化$\hat \mu =0$

  * 定义$c(\hat\mu )$为当前的相关系数
    $$
    c=c(\hat\mu)=X^Tr=X^T(y-\hat \mu)
    $$
    其中$c_j$是正相关于$\mathbf{x}_j$（$X$的第$j$列）和当前残差$r=(y-\hat\mu)$的相关度

  * 下一步就是往相关系数最大的维度方向走一小步
    $$
    \hat j=\arg\max_{j}(|c_j|)\\
    \hat\mu =\hat\mu +\epsilon sign(c_{\hat j})\mathbf{x}_j
    $$
    其中 $\epsilon$ 是一个很小的常数——很小是很有必要的，如果很大的话就容易错过一些中间状态

* 不同之处：当一个变量添加到模型中其他的变量不需要调整

  * 这导致，forward stage 需要用多于 $p$ 步达到最小二乘拟合，历史上也因此被放弃了．但这种“慢拟合”高维问题中会有很好的结果

## 3 例子

* 如何进行选择参数：把这些数据分成了规模为 67 的训练集和规模为 30 的测试集．对训练集进行交叉验证，因为选择收缩参数是训练过程的一部分

* 如何估计均方误差：

  * 由于均方误差和预测误差只差一个$\sigma^2$， 所以直接采样预测误差估计
  * 直接在测试集上测试，取预测误差的均值即可

* 预测值标准误差的含义：

  * 预测值标准误差并不一定是下降的，有可能是先下降后上升的（后面参数过多可逆造成标准误差增大而偏差减小）。

  